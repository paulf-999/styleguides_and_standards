---
variables:
    # the source branch from the Git PR (pull request)
    SOURCE_GIT_BRANCH_NAME: $[replace(variables['System.PullRequest.SourceBranch'], 'refs/heads/', '')]

    # filepath to where the CICD files are stored within the git repo
    CICD_FILES_DIR: src/cicd/dbt_slim_ci_job
    # dbt variables
    DBT_PROJECT_DIR: prod_analytics
    DBT_PROFILE_DIR: prod_analytics/profiles
    DBT_PROD_RUN_ARTIFACTS_DIR: tmp/dbt_prod_run_artifacts
    # Azure Key Vault name
    AZURE_KEY_VAULT_NAME: dmt-uat-uscn-oos-kv

# only trigger the CI job on PR (pull request) to the master branch
pr:
    branches:
        include:
            - master
    # Also only trigger the CI job when changes are made to the following directories/files
    paths:
        include:
            - prod_analytics/data/*
            - prod_analytics/models/*
            - prod_analytics/snapshots/*
            - prod_analytics/tests/*
            - prod_analytics/dbt_project.yml

pool: onprem-linux-elksdx

jobs:
    #=========================================================================================================================================
    # Job 1: Validate the Git branch name
    # Ensure the Git branch name follows the naming standard (i.e., uses underscores & not hyphens) - ^(feature|hotfix|release)\/[a-z0-9_]+$
    #=========================================================================================================================================
    - job: validateGitBranchName
      displayName: Git branch name validation
      steps:
          - task: Bash@3
            inputs:
                filePath: src/cicd/git_branch_name_validation.sh
                arguments: $(SOURCE_GIT_BRANCH_NAME)
            displayName: Validate Git branch name
    #======================================================================================================================================
    # Job 2: dbt run modified & downstream models only
    # Prerequisite: to generated a value for 'state:modified', we require two manifest.json files for comparison - master vs feature branch
    #======================================================================================================================================
    - job: dbtSlimCIJob
      displayName: dbt 'Slim CI' job
      steps:
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Note: the first 3 steps below (i.e., before 'Step 1') are to be removed once the AZ agent upgrade has been made.
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: rm -rf /usr/local/lib/python3.8/dist-packages/OpenSSL && rm -rf /usr/local/lib/python3.8/dist-packages/pyOpenSSL-22.1.0.dist-info/
            displayName: Prep step/too be removed - Remove the OpenSSL package files (it's causing issues)
          - script: pip install --upgrade pyOpenSSL
            displayName: Prep step/too be removed - Upgrade pyOpenSSL
          - script: pip install -r $(CICD_FILES_DIR)/requirements.txt
            displayName: Prep step/to be removed - pip install requirements.txt
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 1: Connect to Azure Key Vault (KV) and fetch the Vault secrets we're interested in
          #--------------------------------------------------------------------------------------------------------------------------------------
          - task: AzureKeyVault@2
            inputs:
                azureSubscription: pyrc-uat-oos-sb
                KeyVaultName: $(AZURE_KEY_VAULT_NAME)
                SecretsFilter: '*'
                RunAsPreJob: true
            displayName: AZ Key Vault - Retrieve Secrets
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 2: Generate a timestamp var ($DATETIME_TS) to use as a unique schema name
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: |
                echo && echo "Generate a datetime timestamp var - this is then used in the dbt profile render step (below)" && echo
                date=$(date +%Y%m%d_%H%M%S)
                echo "##vso[task.setvariable variable=DATETIME_TS;]$date"
            displayName: Generate timestamp var (i.e., $DATETIME_TS) for dynamic schema name
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 3: Using the above KV secrets, let's render the dbt 'QA' profiles.yml file
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: |
                echo && echo "##[section]Step 1: Render the dbt qa_profiles.yml template."
                echo "##[debug]Command = j2 src/templates/qa_profiles.yml.j2 -o $(DBT_PROFILE_DIR)/profiles.yml"
                j2 src/templates/qa_profiles.yml.j2 -o $(DBT_PROFILE_DIR)/profiles.yml
                echo "##[section]Step 2: Print out the (masked) contents of the generated profiles.yml file" && echo
                cat $(DBT_PROFILE_DIR)/profiles.yml && echo
            env:
                SNOWFLAKE_ACCOUNT_NAME: $(SNOWFLAKE-ACCOUNT-NAME)
                SNOWFLAKE_ROLE_QA: $(SNOWFLAKE-ROLE-QA)
                SNOWFLAKE_DBT_USERNAME_QA: $(SNOWFLAKE-DBT-USERNAME-QA)
                SNOWFLAKE_DATABASE_QA: $(SNOWFLAKE-DATABASE-QA)
                DATETIME_STRING: $(DATETIME_TS) # Created in the above step "Timestamp VAR Creation (i.e., $DATETIME_TS)"
                SNOWFLAKE_SCHEMA_PREFIX_QA: $(SNOWFLAKE-SCHEMA-PREFIX-QA)
            displayName: Render dbt profiles.yml
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 4. In case the job doesn't complete & we want to troubleshoot the job failures, let's push the compiled (dbt) SQL files to the azure pipeline assets
          #--------------------------------------------------------------------------------------------------------------------------------------
          # copy the dbt generated artefacts (i.e., those stored within the dbt 'target' directory)
          # though to generate the compiled dbt models, we'll first need to compile our dbt project
          - script: cd $(DBT_PROJECT_DIR) && dbt --quiet compile --profiles-dir=profiles
            env:
                SNOWFLAKE_DBT_PASSWORD_QA: $(SNOWFLAKE-DBT-PASSWORD-QA)
            displayName: Troubleshooting - run 'dbt compile' to generate the rendered dbt compilation files
          # part 1: copy the compiled dbt models
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/target/compiled/prod_analytics/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt target/compiled artefacts (target/run/prod_analytics)
          # part 2: copy the dbt generated logs
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/logs/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt logs
          # publish the logs & compiled dbt models to the azure pipeline assets
          - task: PublishBuildArtifacts@1
            inputs:
                PathtoPublish: $(Build.ArtifactStagingDirectory)
                ArtifactName: dbt_job_run_artifacts
                publishLocation: Container
            displayName: Publish the dbt logs & artifacts
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 5: Run modified (and downstream) dbt models
          #--------------------------------------------------------------------------------------------------------------------------------------
          - task: Bash@3
            inputs:
                filePath: $(CICD_FILES_DIR)/dbt_slim_ci_job.sh
                arguments: $(SOURCE_GIT_BRANCH_NAME)
            env:
                SNOWFLAKE_DBT_PASSWORD_QA: $(SNOWFLAKE-DBT-PASSWORD-QA)
            displayName: === Main dbt 'Slim CI' job logic === (see src/cicd/dbt_slim_ci_job/dbt_slim_ci_job.sh)
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 6: Copy the dbt logs & the dbt run artefacts (i.e., those stored within the dbt 'target' directory)
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Part 1: copy the dbt generated logs
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/logs/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt logs

          # Part 2: copy the dbt run artefacts (i.e., those stored within the dbt 'target' directory)
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/target/run/prod_analytics/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt target/run artefacts (target/run/prod_analytics)

          # Part 2: copy the list of modified dbt models identified
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/dbt_modified_models.txt
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy the list of modified dbt models identified
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 7. Let's now push all of our dbt assets (logs & compiled/'run' dbt models) up to AZ artefacts
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Let's again push the dbt generated logs up to AZ artefacts
          - task: PublishBuildArtifacts@1
            inputs:
                PathtoPublish: $(Build.ArtifactStagingDirectory)
                ArtifactName: dbt_job_run_artifacts
                publishLocation: Container
            displayName: Publish the dbt logs & artifacts
    #=========================================================================================================================================
    # Job 3: Lint the pushed SQL code using SQLFluff.
    # Perform a SQLFluff lint on new/modified dbt models only. Where the 'dbt' templater has been used.
    #=========================================================================================================================================
    - job: SQLFluff
      # only run SQLFluff lint if the dbt 'QA' job ran. As we only want to run SQLFluff if new files were added.
      # dependsOn: dbtQAJob
      displayName: SQLFluff lint
      steps:
          - script: |
                # check if the number of modified dbt models in this CI run is empty
                if [ ! -s "dbt_modified_models.txt" ]; then
                    echo && echo "#-----------------------------------------------------------------------------------------------------------------------------------------------------"
                    echo "# The file is empty (i.e., no dbt models were modified). Skip SQLFluff test job."
                    echo "##vso[task.setvariable variable=isFileEmpty;isOutput=true]true"
                    echo "#-----------------------------------------------------------------------------------------------------------------------------------------------------"
                else
                    echo "The file is not empty."
                    echo "##vso[task.setvariable variable=isFileEmpty;isOutput=true]false"
                    echo "The number of lines in the file is: $LINE_COUNT"
                fi
            displayName: Check if file is empty
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Note: the first 3 steps below (i.e., before 'Step 1') are to be removed once the AZ agent upgrade has been made.
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: rm -rf /usr/local/lib/python3.8/dist-packages/OpenSSL && rm -rf /usr/local/lib/python3.8/dist-packages/pyOpenSSL-22.1.0.dist-info/
            displayName: Prep step/too be removed - Remove the OpenSSL package files (it's causing issues)
            # Only run the subsequent tasks if the input file is NOT empty
            # condition: eq(variables['isFileEmpty'], 'false')
          - script: pip install --upgrade pyOpenSSL
            displayName: Prep step/too be removed - Upgrade pyOpenSSL
            # Only run the subsequent tasks if the input file is NOT empty
            # condition: eq(variables['isFileEmpty'], 'false')
          - script: pip install -r $(CICD_FILES_DIR)/requirements.txt
            displayName: Prep step/to be removed - pip install requirements.txt
            # Only run the subsequent tasks if the input file is NOT empty
            # condition: eq(variables['isFileEmpty'], 'false')
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 1: Connect to Azure Key Vault (KV) and fetch the Vault secrets we're interested in
          #--------------------------------------------------------------------------------------------------------------------------------------
          - task: AzureKeyVault@2
            inputs:
                azureSubscription: pyrc-uat-oos-sb
                KeyVaultName: $(AZURE_KEY_VAULT_NAME)
                SecretsFilter: '*'
                RunAsPreJob: true
            displayName: AZ Key Vault - Retrieve Secrets
            # Only run the subsequent tasks if the input file is NOT empty
            # condition: eq(variables['isFileEmpty'], 'false')
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 2: Generate a timestamp var ($DATETIME_TS) to use as a unique schema name
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: |
                echo && echo "Generate a datetime timestamp var - this is then used in the dbt profile render step (below)" && echo
                date=$(date +%Y%m%d_%H%M%S)
                echo "##vso[task.setvariable variable=DATETIME_TS;]$date"
            displayName: Generate timestamp var (i.e., $DATETIME_TS) for dynamic schema name
            # Only run the subsequent tasks if the input file is NOT empty
            # condition: eq(variables['isFileEmpty'], 'false')
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 3: Using the above KV secrets, let's render the dbt profiles.yml file
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: |
                echo && echo "##[section]Step 1: Render the dbt qa_profiles.yml template."
                echo "##[debug]Command = j2 src/templates/qa_profiles.yml.j2 -o $(DBT_PROFILE_DIR)/profiles.yml"
                j2 src/templates/qa_profiles.yml.j2 -o $(DBT_PROFILE_DIR)/profiles.yml
                echo "##[section]Step 2: Print out the (masked) contents of the generated profiles.yml file" && echo
                cat $(DBT_PROFILE_DIR)/profiles.yml && echo
            env:
                SNOWFLAKE_ACCOUNT_NAME: $(SNOWFLAKE-ACCOUNT-NAME)
                SNOWFLAKE_ROLE_QA: $(SNOWFLAKE-ROLE-QA)
                SNOWFLAKE_DBT_USERNAME_QA: $(SNOWFLAKE-DBT-USERNAME-QA)
                SNOWFLAKE_DATABASE_QA: $(SNOWFLAKE-DATABASE-QA)
                DATETIME_STRING: $(DATETIME_TS) # Created in the above step "Timestamp VAR Creation (i.e., $DATETIME_TS)"
                SNOWFLAKE_SCHEMA_PREFIX_QA: $(SNOWFLAKE-SCHEMA-PREFIX-QA)
            displayName: Render dbt profiles.yml
            # Only run the subsequent tasks if the input file is NOT empty
            # condition: eq(variables['isFileEmpty'], 'false')
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 4: Let's now attempt to use the SQLFluff command
          #--------------------------------------------------------------------------------------------------------------------------------------
          - task: Bash@3
            inputs:
                filePath: src/cicd/sqlfluff_lint.sh
            displayName: === Main SQLFluff step === SQLFluff lint the modified dbt models
            env:
                SNOWFLAKE_DBT_PASSWORD_QA: $(SNOWFLAKE-DBT-PASSWORD-QA)
